---
layout: post
title: Project 2.  Predict Margin of Victory(MOV) in University of Michigan Football
---
For the Project 2 at Metis bootcamp, students had opportunities to select their own topics to build predictive regression models.I decided to predict margin of victory(MOV) in the University of Michigan football games. Since graduating from University of Michigan (UM), I became a big fan of UM sports. Although Michigan is not playing well than we expect, I will always support and cheer for coaches and players as true Michigan fan.

GO BLUE!!

![Image test]({{ site.url }}/images/UM_football.jpg)


## 1. Data Preprocessing

The data was scraped from [website](https://www.sports-reference.com/cfb/) using BeautifulSoup and Pandas. The statistics for both offense and defense from 2000 to present (2019/10/12) were collected. Examples are passing, rushing, total offense, first downs, penalties, and turnovers statistics. Addition to these features, I created Home/Away/Neutral sites and rank/unrank opponents columns. First, some of the irrelevant features were removed by studying correlations. Further study was performed by plotting heatmap and a Figure 1 clearly shows that there are many features having multicolinearity. Since multicolinearity makes the coefficient very unstable, features that have less correlations and high p-values (>0.05) were removed. A Figure 2 shows the new heatmap with six features, showing no multicolinearity. 
![Image test]({{ site.url }}/images/Heatmap.svg)
Figure 1. Heatmap showing multicolinearity.

![Image test]({{ site.url }}/images/Heatmap_2.svg)
Figure 2. Heatmap after removing features having multicolinearity.

A working approach in this study is to find the best model (Linear, Ridge, Polynominal,LASSO) using train and test datasets and then predict future results for the rest of 2019 season games. 

## 2. Determine the best model
Here, four different models, Linear, Ridge, Polynomial, and LASSO regressions, were tested to find the best fit model to predict the future results. First, effective way to split the datasets was studied by analyzing R squared, adjusted R squared, and residual mean squre eroor(RMSE) values. This approach allow for me to find the best model. Then, the dataset was manipulated in such way that allow for future predictions. Explanations for each step are discussed below.

#### 2.1. R and adjusted R squred values comparison in three different splitted datasets
Three main ways to split datasets were considered in this study: 1. 60 train/20 validation/20 test, 2. 80 train/ 20 test, 3. 80 train/ 20 test with cross validation. First, the R squared and adjusted R squared values were calculated to find the best fit model. For each method,the percentage was varied during calculation. The result showed that 80 train/ 20 test with cross validation in Ridge regression showed the fit with he R squared values for train and test of 0.75 and 0.76, respectively. However, all methods had one major problem. The R squared and adjusted R squared values were depend on random state number. Selecting different random state number resulted in different fitting results, under or overfitting models. A Figure 3 shows an example of having fluctuated R squared values for test dataset in different random state number. This problem arises from small datasets and analyzing only R squared values were not effectively way to find the best model in this study. 
![Image test]({{ site.url }}/images/R2_plot.svg)
Figure 3. R squared vs. random state number for train and test dataset showing comparing R squared is not practical way to find the best model.

#### 2.2. Residual Mean Squared Error(RMSE)
Residual Mean Squared error(RMSE) measures the standard deviation of the residuals (prediction errors). In other words,it shows how close the data are to the fitted regression model. RMSE of 9.8 with standard deviation of 1 was calcualted for Ridge regression model. This means about 10 points can be different from the analysis. This was the best result among other models. RMSE of 9.8 showed a fairly good agreement because the number of touchdown was most influential features in this study. Here, an assumption was made that extra kick is always successful, resulting 7 points for each touchdown. Based on the analysis that have done so far, MOV predictions for the rest of 2019 games were studied.

## 3. Prediction for the rest of 2019 games

The dataset was manipulated as shown in Table 1. By grouping by opponents, Home/Away, and date,  THE MOV columns were shifted in such way that predicts for the future result. 

Table 1. An example of manipulating dataset for future prediction.
![Image test]({{ site.url }}/images/Future_dataset.png)

The Ridge regression showed the best result and a Table 2 showed the predictions with comparisons to the [ betting website](https://www.teamrankings.com). The results are in reasonably good agreement, but R squred value was very low, 0.23 with high RMSE of about 17.  

Table2. Predictions for the rest of 2019 games (+ = Michigan is favor to win)
![Image test]({{ site.url }}/images/Predict_Future.png)

This showed that the current model is not practical to predict the results for the future games. However, the model can be improved by taking considerations of coach changes, impact players departing from graduating or NFL drafts, and adding more features like field goals.